{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments, RobertaModel, RobertaPreTrainedModel, BertModel, BertPreTrainedModel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrieval_with_Faiss:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        q_encoder,\n",
    "        p_encoder=None,\n",
    "        num_neg=5,\n",
    "        hard_neg=1,\n",
    "        is_trained=False,\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            if p_encoder is not None:\n",
    "                self.p_encoder.cuda()\n",
    "            self.q_encoder.cuda()\n",
    "        \n",
    "        self.wiki_dataset = load_from_disk(\"/home/ubuntu/workspace/data/wiki_preprocessed_droped\")\n",
    "        \n",
    "        if is_trained:\n",
    "            pass\n",
    "        else:\n",
    "            self.prepare_in_batch_negative(num_neg=num_neg, hard_neg=hard_neg)\n",
    "\n",
    "    def prepare_in_batch_negative(self,\n",
    "        dataset=None,\n",
    "        num_neg=4,\n",
    "        hard_neg=1,\n",
    "        k=100,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        if num_neg < hard_neg:\n",
    "            raise 'num_neg는 hard_neg보다 커야합니다.'\n",
    "        wiki_datasets = self.wiki_dataset\n",
    "        wiki_datasets.load_elasticsearch_index(\"text\", host=\"localhost\", port=\"9200\", es_index_name=\"wikipedia_contexts\")\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # 1. In-Batch-Negative 만들기\n",
    "        p_with_neg = []\n",
    "\n",
    "        for c in tqdm(dataset):\n",
    "            p_with_neg.append(c['context'])\n",
    "            query = c['question']\n",
    "            p_neg = []\n",
    "            _, retrieved_examples = wiki_datasets.get_nearest_examples(\"text\", query, k=k)\n",
    "            for index in range(k):\n",
    "                if retrieved_examples['document_id'][index] == c['document_id']:\n",
    "                    continue\n",
    "                p_neg.append(retrieved_examples['text'][index])\n",
    "            p_with_neg.extend(p_neg[5:5+hard_neg])\n",
    "            p_with_neg.extend(random.sample(p_neg[50:], num_neg - hard_neg))\n",
    "            assert len(p_with_neg) % (num_neg + 1) == 0, '데이터가 잘못 추가되었습니다.'\n",
    "\n",
    "        # 2. (Question, Passage) 데이터셋 만들어주기\n",
    "        q_seqs = tokenizer(\n",
    "            dataset[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        p_seqs = tokenizer(\n",
    "            p_with_neg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg+1, max_len)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "    def build_faiss(self, del_p_encoder=True):\n",
    "        eval_batch_size = 8\n",
    "\n",
    "        self.search_corpus = list(set([example['text'] for example in self.wiki_dataset]))[:100]\n",
    "        \n",
    "        # Construt dataloader\n",
    "        valid_p_seqs = self.tokenizer(\n",
    "            self.search_corpus,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        valid_dataset = TensorDataset(\n",
    "            valid_p_seqs[\"input_ids\"],\n",
    "            valid_p_seqs[\"attention_mask\"],\n",
    "            valid_p_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        valid_sampler = SequentialSampler(valid_dataset)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler=valid_sampler,\n",
    "            batch_size=eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Inference using the passage encoder to get dense embeddeings\n",
    "        p_embs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            epoch_iterator = tqdm(\n",
    "                valid_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                position=0,\n",
    "                leave=True\n",
    "            )\n",
    "            self.p_encoder.eval()\n",
    "\n",
    "            for _, batch in enumerate(epoch_iterator):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2]\n",
    "                }\n",
    "                \n",
    "                outputs = self.p_encoder(**p_inputs).to(\"cpu\").numpy()\n",
    "                p_embs.extend(outputs)\n",
    "                \n",
    "        # 이제 p encoder 쓸 일이 없을경우 삭제        \n",
    "        if del_p_encoder:\n",
    "            del self.p_encoder\n",
    "            \n",
    "        p_embs = np.array(p_embs)\n",
    "        emb_dim = p_embs.shape[-1]\n",
    "\n",
    "        cpu_index = faiss.IndexFlatL2(emb_dim)  # Flat에 GPU 사용\n",
    "        self.indexer = faiss.index_cpu_to_all_gpus(cpu_index)\n",
    "        self.indexer.add(p_embs)\n",
    "        faiss.write_index(faiss.index_gpu_to_cpu(self.indexer), 'wiki.index')\n",
    "\n",
    "    def train(self, args=None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        batch_size = args.per_device_train_batch_size\n",
    "\n",
    "        # Optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon\n",
    "        )\n",
    "        t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Start training!\n",
    "        self.p_encoder.train()\n",
    "        self.q_encoder.train()\n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\")\n",
    "        # for _ in range(int(args.num_train_epochs)):\n",
    "        for _ in train_iterator:\n",
    "\n",
    "            with tqdm(self.train_dataloader, unit=\"batch\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "            \n",
    "                    targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                    targets = targets.to(args.device)\n",
    "\n",
    "                    p_inputs = {\n",
    "                        \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"token_type_ids\": batch[2].view(batch_size * (self.num_neg + 1), -1).to(args.device)\n",
    "                    }\n",
    "            \n",
    "                    q_inputs = {\n",
    "                        \"input_ids\": batch[3].to(args.device),\n",
    "                        \"attention_mask\": batch[4].to(args.device),\n",
    "                        \"token_type_ids\": batch[5].to(args.device)\n",
    "                    }\n",
    "\n",
    "                    # (batch_size*(num_neg+1), emb_dim)\n",
    "                    p_outputs = self.p_encoder(**p_inputs)\n",
    "                    # (batch_size*, emb_dim)\n",
    "                    q_outputs = self.q_encoder(**q_inputs)\n",
    "\n",
    "                    # Calculate similarity score & loss\n",
    "                    p_outputs_t = torch.transpose(p_outputs.view(batch_size, self.num_neg + 1, -1), 1 , 2)\n",
    "                    q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                    sim_scores = torch.bmm(q_outputs, p_outputs_t).squeeze()  #(batch_size, num_neg + 1)\n",
    "                    sim_scores = sim_scores.view(batch_size, -1)\n",
    "                    sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                    loss = F.nll_loss(sim_scores, targets)\n",
    "                    tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    self.p_encoder.zero_grad()\n",
    "                    self.q_encoder.zero_grad()\n",
    "\n",
    "                    global_step += 1\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    del p_inputs, q_inputs\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1):\n",
    "        valid_q_seqs = self.tokenizer(query, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.q_encoder.eval()\n",
    "            q_emb = self.q_encoder(**valid_q_seqs).to(\"cpu\").numpy()\n",
    "            del valid_q_seqs\n",
    "        \n",
    "        q_emb = q_emb.astype(np.float32)\n",
    "        D, I = self.indexer.search(q_emb, k)\n",
    "        distances, index = D.tolist()[0], I.tolist()[0]\n",
    "        \n",
    "        distance_list, doc_list = [], []\n",
    "        for d, i in zip(distances, index):\n",
    "            distance_list.append(d)\n",
    "            doc_list.append(self.search_corpus[i])\n",
    "\n",
    "        return distance_list, doc_list, D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEncoder, self).__init__(config)\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__', 'chunks'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가를 안하니 검증데이터와 훈련데이터를 합칩니다.\n",
    "train_dataset = load_from_disk('/home/ubuntu/workspace/data/train_dataset')\n",
    "train = train_dataset['train'].to_dict()\n",
    "valid = train_dataset['validation'].to_dict()\n",
    "for key in train.keys():\n",
    "  train[key].extend(valid[key])\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "train_dataset = train_dataset.select(list(range(50)))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1cb5d5b2a04c77ac22cc99eb55ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/mrc_venv/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "retriever = DenseRetrieval_with_Faiss(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3847b28b444fd682ed49169c543228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819bf64ff8344e91886fba7640998964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c02a237b27e4d1ab5ad5d7dadc3e08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfa1a8758014030ab16b85d6dead8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5b0997cf71435193addcb0a29bffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.build_faiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "query = \"실잠자리는 무엇을하는가?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['비록 흑점 발생의 세부사항은 아직 연구의 대상이지만 분명히 흑점은 차등회전에 의해 시작된 태양의 대류층 안의 자기 플럭스 튜브의 가시적인 카운터파트이다. 만약 플럭스 투브 위의 압력이 특정 한계에 도달하면 그들은 고무 밴드와 같이 말아올라가고 태양 표면을 뚫는다. 펑크점에서 대류는 막히고 태양 내부에서 나오는 에너지 흐름은 감소하며 그에 따라 표면 온도도 떨어진다. 윌슨 효과는 우리에게 흑점이 실제로는 태양 표면위의 저기압(depression)이라고 말해 준다. 쉽게 설명하자면 태양의 자전속도가 빨라지면서 태양의 자기장이 영향을 받아, 꼬이고 엉키면서 한 지점에서 집중적으로 자기장이 강한 부분이 생겨나게 되고, 강한 자기장으로 인해 태양의 대류가 지체가 되고 온도가 낮아지면서 흑점이 생겨나는 것이다.',\n",
       " '변광성 일반 목록에는 장주기 변광성이 정의되어 있지 않지만, 미라형 변광성은 주기가 긴 변광성으로 서술되어 있다 이 분류는 처음에 주기가 몇백 일 단위로 매우 긴 변광성을 분류하기 위해 사용되었다 20세기 중반에, 장주기 변광성의 정체가 차가운 거성으로 거의 확정되었고 미라형 변광성과 가까운 반규칙 변광성 등 변광성 전체에 대해 다시금 연구가 이루어져 \"장주기 변광성\"이라는 분류가 생겨나게 되었다. 반규칙 변광성은 장주기 변광성과 세페이드 변광성의 중간으로 여겨진다 변광성 일반 목록 출판 후, 미라형 및 반규칙 변광성(중 SRa)은 간혹 장주기 변광성으로 간주되었다 장주기 변광성은 넓게 보면 미라형, 반규칙, 저속 불규칙 변광성, OGLE 소진폭 적색거성(OSARGs)으로 볼 수 있지만 OSARGs는 일반적으로 장주기 변광성으로 취급되지 않으며 연구자 대부분은 장주기 변광성을 미라형 및 반규칙, 또는 미라형만으로 보고 있다 미국 변광성 관측자 협회의 장주기 변광성 문단에서는 \"미라형, 반규칙, 황소자리 RV형 등 모든 적색거성\"들을 다루고 있다 황소자리 RV형 변광성에는 SRc형(반규칙)과 Lc형(불규칙) 적색 거성이 포함된다. 최근 연구에서는 점근거성가지에 대한 연구에 초점을 두고 있다',\n",
       " '실제 실험에 근거하여, 백금과의 반응은 금에 비해 훨씬 더 복잡하다. 금과 마찬가지로 반응에서 일산화 질소와 이산화 질소가 생성된다. :Pt + 4 NO + 8 H → Pt + 4 NO + 4 HO :3 Pt + 4 NO + 16 H → 3 Pt + 4 NO + 8 HO 산화된 백금 이온은 다음과 같이 염화 이온과 반응하여 염화 백금산 이온(IV)(PtCl)을 만든다. :Pt + 6 Cl → PtCl 초기 반응에서는 사염화 백금산(IV)(HPtCl)과 염화 니트로소 백금((NO)(NO)PtCl)이 만들어진다. 백금을 완전히 용해시켜야 하는 경우 고농도의 염산으로 잔류 부산물들을 여러 번 제거해야된다. :2 Pt + 2HNO + 8 HCl → (NO)PtCl + HPtCl + 4 HO 또는 :(NO)PtCl + 2 HCl ⇌ HPtCl + 2 NOCl 염소 기체를 사염화 백금산(IV)용액에 포화시킨 뒤 가열하면 육염화 백금산(VI)(HPtCl)을 얻을 수 있다. :HPtCl + Cl → HPtCl',\n",
       " '루시퍼레이즈(Luciferase)는 발광(light-emitting) 반응을 촉매하는 효소를 부르는 일반적인 이름이다. 루시퍼레이즈는 세균, 조류, 진균류, 해파리, 곤충, 새우, 오징어 등에서 발견되며 이런 생물이 만들어 내는 빛을 생물 발광(bioluminescence)라고 한다. 세균에서 분리된 이러한 발광 반응을 담당하는 유전자는 490 nm에서 최대 강도를 보이는 푸른(blue-green) 빛을 내는 바이오리포터를 만드는데 널리 사용된다. 3가지 lux 변종이 사용가능하며, 각각 30C 이하, 37C 이하, 45C 이하에서 기능한다. lux 유전자 시스템은 luxA, luxB, luxC, luxD, 그리고 luxE의 다섯가지 유전자 부위로 구성되어 있다. 이 유전자의 조합에 따라서, 몇가지 다른 형태의 바이오 리포터가 사용될 수 있다.',\n",
       " '루팅(rooting)은 모바일 기기에서 구동되는 안드로이드 운영 체제 상에서 최상위 권한(루트 권한)을 얻음으로 해당 기기의 생산자 또는 판매자 측에서 걸어 놓은 제약을 해제하는 행위를 가리키는 말이다. 이 루팅을 통해 해당 기기의 사용자들은 생산자 또는 판매자 측에서 공식 제공하는 버전보다 더 높은 버전의 안드로이드나 CyanogenMod처럼 사용자들이 임의로 개조한 안드로이드를 설치 및 구동할 수 있으며, 사용자가 속한 지역의 안드로이드 사용자들에게 판매하지 않는 프로그램들을 구입하거나 일반 사용자 권한 이상의 권한 등을 필요로 하는 프로그램들을(백업 프로그램, 하드웨어 해킹 프로그램 등) 사용할 수 있다. 안드로이드 특성상 반드시 최고권한(관리자)이 필요한 동작(카메라무음, 파일접근, 시스템앱삭제 등등)을 수행하고자 할 때 진행된다. 한 번 루팅된 안드로이드는 언루팅 하지 않는 이상 몇 번이고 supersu를 지웠다 설치했다 해도 루팅은 그대로 유지된다. 기기의 생산자 또는 판매자 측에서 걸어 놓은 제약을 해제한다는 면에서 iOS 관련 용어인 탈옥(Jailbreaking)과 비슷하지만 안드로이드 루팅은 서드파티 프로그램 설치를 막아 놓은 AT&T 안드로이드 폰들을 제외하고는 루팅 없이도 구글 플레이에서 제공하지 않는 프로그램을 따로 구할 필요가 없다는 차이점이 있다.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_encoder.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/tokenizer_config.json',\n",
       " 'model/special_tokens_map.json',\n",
       " 'model/vocab.txt',\n",
       " 'model/added_tokens.json',\n",
       " 'model/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('model')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac68cbaab3d35a4051d1e6f867fd4af3051b6908d059b1a63e6857e32e52681"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('mrc_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
