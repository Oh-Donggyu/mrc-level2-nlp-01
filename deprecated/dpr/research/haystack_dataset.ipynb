{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_from_disk, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = load_from_disk(\"/opt/ml/data/wiki_preprocessed_droped\")\n",
    "train_dataset = load_from_disk(\"/opt/ml/data/train_dataset\")\n",
    "wiki_datasets.load_elasticsearch_index(\"text\", host=\"localhost\", port=\"9200\", es_index_name=\"wikipedia_contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55962/55962 [00:09<00:00, 5988.33it/s]\n"
     ]
    }
   ],
   "source": [
    "dicts = []\n",
    "for wiki in tqdm(wiki_datasets):\n",
    "  wiki_dick = {}\n",
    "  wiki_dick['content'] = wiki['text']\n",
    "  wiki_dick['meta'] = {\n",
    "    'title': wiki['title'],\n",
    "    'document_id': wiki['document_id']\n",
    "  }\n",
    "  dicts.append(wiki_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic search 기반 데이터셋 제작\n",
    "\n",
    "def generate_dpr_dataset(target_dataset, dataset_name):\n",
    "  dpr_train_datas = []\n",
    "  def change_score(x):\n",
    "    x['score'] = 0\n",
    "    return x\n",
    "  for data in tqdm(target_dataset):\n",
    "    train_dict = {}\n",
    "    train_dict['dataset'] = dataset_name\n",
    "    train_dict['question'] = data['question']\n",
    "    train_dict['answers'] = data['answers']['text']\n",
    "    train_dict['positive_ctxs'] = [{\n",
    "      'title': data['title'],\n",
    "      'text': data['context'],\n",
    "      'score': 1000,\n",
    "      'title_score': 1,\n",
    "      'passage_id': data['document_id']\n",
    "    }]\n",
    "    negatives = []\n",
    "    query = data['question']\n",
    "    scores, retrieved_examples = wiki_datasets.get_nearest_examples(\"text\", query, k=100)\n",
    "    for index in range(100):\n",
    "      if retrieved_examples['document_id'][index] == data['document_id']:\n",
    "        continue\n",
    "      negative_dict = {\n",
    "        'title': retrieved_examples['title'][index],\n",
    "        'text': retrieved_examples['text'][index],\n",
    "        'score': scores[index],\n",
    "        'title_score': 0,\n",
    "        'passage_id': retrieved_examples['document_id'][index]\n",
    "      }\n",
    "      negatives.append(negative_dict)\n",
    "    train_dict['hard_negative_ctxs'] = random.sample(negatives[:15], 5)\n",
    "    train_dict['negative_ctxs'] = list(map(change_score, random.sample(negatives[50:], 10)))\n",
    "    dpr_train_datas.append(train_dict)\n",
    "  return dpr_train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dpr_dataset(target_dataset, dataset_name):\n",
    "  dpr_train_datas = []\n",
    "  for data in tqdm(target_dataset):\n",
    "    train_dict = {}\n",
    "    train_dict['dataset'] = dataset_name\n",
    "    train_dict['question'] = data['question']\n",
    "    train_dict['answers'] = data['answers']['text']\n",
    "    train_dict['positive_ctxs'] = [{\n",
    "      'title': data['title'],\n",
    "      'text': data['context'],\n",
    "      'score': 1000,\n",
    "      'title_score': 1,\n",
    "      'passage_id': data['document_id']\n",
    "    }]\n",
    "    negatives = []\n",
    "    query = data['question']\n",
    "    scores, retrieved_examples = wiki_datasets.get_nearest_examples(\"text\", query, k=100)\n",
    "    for index in range(10):\n",
    "      if retrieved_examples['document_id'][index] == data['document_id']:\n",
    "        continue\n",
    "      negative_dict = {\n",
    "        'title': retrieved_examples['title'][index],\n",
    "        'text': retrieved_examples['text'][index],\n",
    "        'score': scores[index],\n",
    "        'title_score': 0,\n",
    "        'passage_id': retrieved_examples['document_id'][index]\n",
    "      }\n",
    "      negatives.append(negative_dict)\n",
    "    train_dict['hard_negative_ctxs'] = random.sample(negatives, 1)\n",
    "    negative_index = random.sample(list(range(len(wiki_datasets))), 20)\n",
    "    negative_list = wiki_datasets[negative_index]\n",
    "    negatives = []\n",
    "    for index in range(20):\n",
    "      if negative_list['document_id'][index] == data['document_id']:\n",
    "        continue\n",
    "      if negative_list['document_id'][index] in map(lambda x: x['passage_id'],train_dict['hard_negative_ctxs']):\n",
    "        continue\n",
    "      negatives.append({\n",
    "        'title': negative_list['title'][index],\n",
    "        'text': negative_list['text'][index],\n",
    "        'score': 0,\n",
    "        'title_score': 0,\n",
    "        'passage_id': negative_list['document_id'][index]\n",
    "      })\n",
    "    train_dict['negative_ctxs'] = random.sample(negatives, 10)\n",
    "    dpr_train_datas.append(train_dict)\n",
    "  return dpr_train_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [07:02<00:00,  9.36it/s]\n",
      "100%|██████████| 240/240 [00:26<00:00,  9.10it/s]\n"
     ]
    }
   ],
   "source": [
    "dpr_train_datas = generate_dpr_dataset(train_dataset['train'], 'original_train')\n",
    "dpr_valid_datas = generate_dpr_dataset(train_dataset['validation'], 'original_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train.json', 'w', encoding='UTF-8') as file:\n",
    "  file.write(json.dumps(dpr_train_datas, ensure_ascii=False))\n",
    "with open('valid.json', 'w', encoding='UTF-8') as file:\n",
    "  file.write(json.dumps(dpr_valid_datas, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dicts.json', 'w', encoding='UTF-8') as file:\n",
    "  file.write(json.dumps(dicts, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
