{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "\n",
    "class KonlpyTokenize:\n",
    "    def __init__(self):\n",
    "        self.noun_collector = Komoran()\n",
    "\n",
    "    def __pre_regex(self, context):\n",
    "        re_compile = re.compile(\"[^a-zA-Z0-9ㄱ-ㅣ가-힣\\s\\(\\)\\[\\]?!.,\\@\\*\\{\\}\\-\\_\\=\\+]\")\n",
    "        context = re.sub(\"\\s\", \" \", context)\n",
    "        re_context = re_compile.sub(\" \", context)\n",
    "        return re_context\n",
    "\n",
    "    def __pre_devide(self, context):\n",
    "        if len(context) < 3000:\n",
    "            return [context]\n",
    "        else:\n",
    "            return re.split(\".\\s|.\\\\n\", context)\n",
    "\n",
    "    def __context_tokenize(self, context):\n",
    "        tokenized_list = []\n",
    "        context = context.strip()\n",
    "        if context == \"\":\n",
    "            return tokenized_list\n",
    "        noum_tokenize = self.noun_collector.pos(context)\n",
    "        for word, tag in noum_tokenize:\n",
    "            if tag == \"NNG\" or tag == \"NNP\":\n",
    "                tokenized_list.append(word)\n",
    "        return tokenized_list\n",
    "\n",
    "    def tokenize_fn(self, context):\n",
    "        context = self.__pre_regex(context)\n",
    "        tokenized_list = []\n",
    "        context_list = self.__pre_devide(context)\n",
    "        for context in context_list:\n",
    "            tokenized_list.extend(self.__context_tokenize(context))\n",
    "        return tokenized_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Tuple, NoReturn, Any, Optional, Union\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{name}] done in {time.time() - t0:.3f} s\")\n",
    "\n",
    "\n",
    "class BM25SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        data_path: Optional[str] = \"/opt/ml/data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "    ) -> NoReturn:\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.bm25 = None\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "\n",
    "    def get_sparse_embedding(self, pickle_name=\"bm25api.bin\") -> NoReturn:\n",
    "\n",
    "        pickle_name = pickle_name\n",
    "        emd_path = os.path.join(self.data_path, pickle_name)\n",
    "\n",
    "        if os.path.isfile(emd_path):\n",
    "            with open(emd_path, \"rb\") as file:\n",
    "                self.bm25 = pickle.load(file)\n",
    "            print(\"Embedding pickle load.\")\n",
    "        else:\n",
    "            print(\"Build passage embedding\")\n",
    "            tokenized_contexts = list(map(self.tokenize_fn, tqdm(self.contexts)))\n",
    "            self.bm25 = BM25Okapi(tokenized_contexts)\n",
    "            with open(emd_path, \"wb\") as file:\n",
    "                pickle.dump(self.bm25, file)\n",
    "            print(\"Embedding pickle saved.\")\n",
    "\n",
    "    def retrieve(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        assert self.bm25 is not None, \"get_sparse_embedding() 메소드를 먼저 수행해줘야합니다.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc(query_or_dataset, k=topk)\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(f\"Top-{i+1} passage with score {doc_scores[i]:4f}\")\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            total = []\n",
    "            doc_scores, doc_indices = [], []\n",
    "            with timer(\"query exhaustive search\"):\n",
    "                for question in tqdm(query_or_dataset[\"question\"]):\n",
    "                    doc_score, doc_indice = self.get_relevant_doc(question, k=topk)\n",
    "                    doc_scores.append(doc_score)\n",
    "                    doc_indices.append(doc_indice)\n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context_id\": doc_indices[idx],\n",
    "                    \"context\": \" \".join(doc_indices[idx]),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            cqas = pd.DataFrame(total)\n",
    "            return cqas\n",
    "\n",
    "    def get_relevant_doc(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "        tokenized_query = self.tokenize_fn(query)\n",
    "        raw_doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "        doc_scores_index_desc = np.argsort(-raw_doc_scores)\n",
    "        doc_scores = raw_doc_scores[doc_scores_index_desc]\n",
    "\n",
    "        doc_list = self.bm25.get_top_n(tokenized_query, self.contexts, k)\n",
    "\n",
    "        return doc_scores[:k], doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_fn = KonlpyTokenize().tokenize_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n"
     ]
    }
   ],
   "source": [
    "retrieval = BM25SparseRetrieval(tokenize_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build passage embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d753c9d1ddaf4834af892e474b7caa30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding pickle saved.\n"
     ]
    }
   ],
   "source": [
    "retrieval.get_sparse_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "datasets = load_from_disk(\"/opt/ml/data/new_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6d9823fe00455588ae461b0f71fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 10\n",
    "score = 0\n",
    "wrong_set = []\n",
    "for data in tqdm(datasets['train']):\n",
    "  query = data['question']\n",
    "  scores, retrieved_examples = retrieval.get_relevant_doc(query, k)\n",
    "  if data['context'] in retrieved_examples:\n",
    "    score += 1\n",
    "  else: \n",
    "    wrong_set.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.48224410623695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score/len(datasets['train']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8170062a3d4ce0a1f0af5283bb8ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 10\n",
    "score2 = 0\n",
    "wrong_set2 = []\n",
    "for data in tqdm(datasets['validation']):\n",
    "  query = data['question']\n",
    "  scores, retrieved_examples = retrieval.get_relevant_doc(query, k)\n",
    "  if data['context'] in retrieved_examples:\n",
    "    score2 += 1\n",
    "  else: \n",
    "    wrong_set2.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.82877526753865"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2 / len(datasets['validation']) * 100"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ac241b0038c08a55f8050af53e8f36f8d3acfd39857d2d9e019009179736117"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
