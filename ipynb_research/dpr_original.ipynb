{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments, RobertaModel, RobertaPreTrainedModel, BertModel, BertPreTrainedModel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwer\n",
    "class DenseRetrieval_with_Faiss:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder,\n",
    "        num_neg=5,\n",
    "        hard_neg=1,\n",
    "        is_trained=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "        \n",
    "        self.wiki_dataset = load_from_disk(\"/home/ubuntu/workspace/data/wiki_preprocessed_droped\")\n",
    "        \n",
    "        if is_trained:\n",
    "            pass\n",
    "        else:\n",
    "            self.prepare_in_batch_negative(num_neg=num_neg, hard_neg=hard_neg)\n",
    "\n",
    "    def prepare_in_batch_negative(self,\n",
    "        dataset=None,\n",
    "        num_neg=5,\n",
    "        hard_neg=1,\n",
    "        k=100,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        if num_neg < hard_neg:\n",
    "            raise 'num_neg는 hard_neg보다 커야합니다.'\n",
    "        wiki_datasets = self.wiki_dataset\n",
    "        wiki_datasets.load_elasticsearch_index(\"text\", host=\"localhost\", port=\"9200\", es_index_name=\"wikipedia_contexts\")\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # 1. In-Batch-Negative 만들기\n",
    "        # CORPUS를 np.array로 변환해줍니다.\n",
    "        p_with_neg = []\n",
    "\n",
    "        for c in tqdm(dataset):\n",
    "            p_with_neg.append(c['context'])\n",
    "            query = c['question']\n",
    "            p_neg = []\n",
    "            _, retrieved_examples = wiki_datasets.get_nearest_examples(\"text\", query, k=k)\n",
    "            for index in range(k):\n",
    "                if retrieved_examples['document_id'][index] == c['document_id']:\n",
    "                    continue\n",
    "                p_neg.append(retrieved_examples['text'][index])\n",
    "            p_with_neg.extend(p_neg[:hard_neg])\n",
    "            p_with_neg.extend(random.sample(p_neg[50:], num_neg - hard_neg))\n",
    "            assert len(p_with_neg) % (num_neg + 1) == 0, '데이터가 잘못 추가되었습니다.'\n",
    "\n",
    "        # 2. (Question, Passage) 데이터셋 만들어주기\n",
    "        q_seqs = tokenizer(\n",
    "            dataset[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        p_seqs = tokenizer(\n",
    "            p_with_neg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg+1, max_len)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "        valid_seqs = tokenizer(\n",
    "            dataset[\"context\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        passage_dataset = TensorDataset(\n",
    "            valid_seqs[\"input_ids\"],\n",
    "            valid_seqs[\"attention_mask\"],\n",
    "            valid_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        self.passage_dataloader = DataLoader(\n",
    "            passage_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "    def build_faiss(self, num_clusters=16):\n",
    "  \n",
    "        \"\"\"\n",
    "        Note:\n",
    "            위에서 Faiss를 사용했던 기억을 떠올려보면,\n",
    "            Indexer를 구성해서 .search() 메소드를 활용했습니다.\n",
    "            여기서는 Indexer 구성을 해주도록 합시다.\n",
    "        \"\"\"\n",
    "        eval_batch_size = 8\n",
    "\n",
    "        self.search_corpus = list(set([example['text'] for example in self.wiki_dataset]))\n",
    "        p_encoder = self.p_encoder\n",
    "        \n",
    "        # Construt dataloader\n",
    "        valid_p_seqs = self.tokenizer(\n",
    "            self.search_corpus,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        valid_dataset = TensorDataset(\n",
    "            valid_p_seqs[\"input_ids\"],\n",
    "            valid_p_seqs[\"attention_mask\"],\n",
    "            valid_p_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        valid_sampler = SequentialSampler(valid_dataset)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler=valid_sampler,\n",
    "            batch_size=eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Inference using the passage encoder to get dense embeddeings\n",
    "        p_embs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            epoch_iterator = tqdm(\n",
    "                valid_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                position=0,\n",
    "                leave=True\n",
    "            )\n",
    "            p_encoder.eval()\n",
    "\n",
    "            for _, batch in enumerate(epoch_iterator):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2]\n",
    "                }\n",
    "                \n",
    "                outputs = p_encoder(**p_inputs).to(\"cpu\").numpy()\n",
    "                p_embs.extend(outputs)\n",
    "        p_embs = np.array(p_embs)\n",
    "        emb_dim = p_embs.shape[-1]\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "        self.indexer = faiss.IndexIVFScalarQuantizer(\n",
    "            quantizer,\n",
    "            quantizer.d,\n",
    "            num_clusters,\n",
    "            faiss.METRIC_L2\n",
    "        )\n",
    "        self.indexer.train(p_embs)\n",
    "        self.indexer.add(p_embs)\n",
    "\n",
    "    def train(self, args=None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        batch_size = args.per_device_train_batch_size\n",
    "\n",
    "        # Optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon\n",
    "        )\n",
    "        t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Start training!\n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\")\n",
    "        # for _ in range(int(args.num_train_epochs)):\n",
    "        for _ in train_iterator:\n",
    "\n",
    "            with tqdm(self.train_dataloader, unit=\"batch\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "\n",
    "                    self.p_encoder.train()\n",
    "                    self.q_encoder.train()\n",
    "            \n",
    "                    targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                    targets = targets.to(args.device)\n",
    "\n",
    "                    p_inputs = {\n",
    "                        \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"token_type_ids\": batch[2].view(batch_size * (self.num_neg + 1), -1).to(args.device)\n",
    "                    }\n",
    "            \n",
    "                    q_inputs = {\n",
    "                        \"input_ids\": batch[3].to(args.device),\n",
    "                        \"attention_mask\": batch[4].to(args.device),\n",
    "                        \"token_type_ids\": batch[5].to(args.device)\n",
    "                    }\n",
    "\n",
    "                    # (batch_size*(num_neg+1), emb_dim)\n",
    "                    p_outputs = self.p_encoder(**p_inputs)\n",
    "                    # (batch_size*, emb_dim)\n",
    "                    q_outputs = self.q_encoder(**q_inputs)\n",
    "\n",
    "                    # Calculate similarity score & loss\n",
    "                    p_outputs_t = torch.transpose(p_outputs.view(batch_size, self.num_neg + 1, -1), 1 , 2)\n",
    "                    q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                    sim_scores = torch.bmm(q_outputs, p_outputs_t).squeeze()  #(batch_size, num_neg + 1)\n",
    "                    sim_scores = sim_scores.view(batch_size, -1)\n",
    "                    sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                    loss = F.nll_loss(sim_scores, targets)\n",
    "                    tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    self.p_encoder.zero_grad()\n",
    "                    self.q_encoder.zero_grad()\n",
    "\n",
    "                    global_step += 1\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    del p_inputs, q_inputs\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (torch.Tensor):\n",
    "                Dense Representation으로 표현된 query를 받습니다.\n",
    "                문자열이 아님에 주의합시다.\n",
    "            k (int, default=1):\n",
    "                상위 몇 개의 유사한 passage를 뽑을 것인지 결정합니다.\n",
    "\n",
    "        Note:\n",
    "            받은 query를 이 객체에 저장된 indexer를 활용해서\n",
    "            유사한 문서를 찾아봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_q_seqs = self.tokenizer(query, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_encoder.eval()\n",
    "            q_emb = q_encoder(**valid_q_seqs).to(\"cpu\").numpy()\n",
    "        \n",
    "        q_emb = q_emb.astype(np.float32)\n",
    "        D, I = self.indexer.search(q_emb, k)\n",
    "        distances, index = D.tolist()[0], I.tolist()[0]\n",
    "        \n",
    "        distance_list, doc_list = [], []\n",
    "        for d, i in zip(distances, index):\n",
    "            distance_list.append(d)\n",
    "            doc_list.append(self.search_corpus[i])\n",
    "\n",
    "        return distance_list, doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEncoder, self).__init__(config)\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__', 'chunks'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가를 안하니 검증데이터와 훈련데이터를 합칩니다.\n",
    "train_dataset = load_from_disk('/home/ubuntu/workspace/data/train_dataset')\n",
    "train = train_dataset['train'].to_dict()\n",
    "valid = train_dataset['validation'].to_dict()\n",
    "for key in train.keys():\n",
    "  train[key].extend(valid[key])\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd14a3f91aa4ce5a63eabde818bc13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/mrc_venv/lib/python3.8/site-packages/elasticsearch/connection/base.py:209: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "retriever = DenseRetrieval_with_Faiss(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9fb66638ef49b8a1793c4f766bd0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe881ae088446d297e2088a933a22ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed208c0616145218ad1bda037b801a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9446754788c0410c911e3b31620b798b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bb1469f38244999f6e02645928e815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656d88aade874ca8ae10757484fac01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_686/506493722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_faiss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_686/1793158232.py\u001b[0m in \u001b[0;36mbuild_faiss\u001b[0;34m(self, num_clusters)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 }\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mp_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mp_embs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mp_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retriever.build_faiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "query = \"리그오브레전드의 선수로, 페이커 데뷔전에서 솔킬을 따인 선수는?\"\n",
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['돈키호테 도플라밍고는 어렸을 때 아버지의 결정으로 아버지, 어머니, 동생 로시난테(코라손)와 함께 성지 마리조아를 떠나 살기 시작했다. 처음에는 이전처럼 부유한 생활을 했지만, 우연히 자신이 천룡인이라는 것을 알게 된 사람들에 의해 집을 잃고 쫓겨다니는 신세가 되었다. 천룡인에 대한 사람들의 증오심은 두려움 못지않게 컸기 때문에, 도플라밍고와 그의 가족은 몇 번이고 죽을 위기를 맞았다. 도플라밍고가 8살이었을 때 어머니가 병을 앓다가 숨을 거두었고, 이 일로 아버지에 대해 분노를 느낀 도플라밍고는 10살 때 아버지를 자기 손으로 죽였다. 그리고 성지 마리조아로 돌아가 다시 천룡인으로 살려고 했으나 그들의 가족은 다른 천룡인들로부터 \\'배신자\\'로 낙인찍혀 두 번 다시 천룡인으로 살 수가 없었다. 이 이후로 도플라밍고는 천룡인을 포함한 전 세계에 대한 복수심을 품고 해적 생활을 시작했다. 해적 활동을 하던 도플라밍고는 천룡인에게 바쳐지는 공물을 나르는 배를 습격해 천상금으로 천룡인을 협박해 세계정부는 그에게 칠무해 자리를 내준다. 칠무해라는 특권인 동시에 족쇄가 되는 이름을 줘야 도플라밍고를 감시할 수 있기 때문이다. 한때 천룡인의 직함을 갖고 있었던 돈키호테 일족인지라 해군본부에서는 해당 인물에 대한 지속적인 관리가 필요했다. 한편으로 칠무해가 되기 전에는 자신의 부하 중 하나인 베르고를 해군에 입대시켜 해군 본부 중장으로 만들어 놓은 다음 베르고로 하여금 해군 본부에서 스파이를 하도록 시켰다. 13년전 코라손이 트라팔가 로에게 해군에게 도움을 요청하라고 했지만 그 많은 해군 중에서 하필이면 베르고에게 도움을 요청하는 바람에 코라손의 계획이 실패한다. 10년 전 칠무해가 된 도플라밍고는 과거에 돈키호테 일족이 다스렸던 드레스로자를 찾아가 국왕인 리쿠 왕에게 나라를 파는 조건으로 100억 베리를 요구한다. 리쿠 왕은 나라를 구하기 위해 국민들에게 돈을 내어 줄 것을 요구하고, 마침내 100억 베리가 모아진다. 하지만 도플라밍고는 리쿠 왕과 드레스로자의 병사들에게 패러사이트를 걸어 사람들을 베도록 조종하고, 사람들이 리쿠 왕에 대해 배반감을 느낄 때 \"이 나라를 구하러 왔다\"고 외치며 일당과 함께 병사들을 몰살한다. 이 일로 도플라밍고는 드레스로자 국민들의 신임을 얻어 드레스로자의 왕이 된다. 드레스로자에서 왕이 된 이후 어느 국민들에게 거짓 친절을 베푸는 반면, 또 어떤 국민들을 장난감으로 만들어 밤낮없이 일을 시키게 하여 어둠의 증오심을 불러일으킨다. 그러던 도중 우솝에 의하여, 장난감들은 본래의 인간으로 돌아가게 된다. 드레스로자에 있던 모든 진실을 모두 감추기 위해 새장을 친뒤 별☆개 1억 베리(레베카, 비올라, 킨에몬, 프랑키, 니코 로빈), 별☆☆개 2억 베리(퀴로스, 롤로노아 조로), 별☆☆☆개 3억 베리(리쿠 돌드 3세, 사보, 트라팔가 로, 몽키 D. 루피)에게 현상금을 걸고, 장난감들을 해방시키고 이 게임을 만든 장본인 우솝에게 별☆☆☆☆☆개 5억 베리를 걸게 만든다. 그러다 자신을 제외한 돈키호테 패밀리가 모두 붕괴되자, 계획을 바꿔 새장을 수축시켜서 모든 국민들을 죽이려다 루피와 싸우게 된다. 한참을 싸웠지만 루피의 기어4 기술을 맞고 고전을 면치 못하다가 결국 루피의 기어 4 킹콩 건을 맞고 쓰러진다. 그 다음에 해군에게 체포된다. 임펠 다운으로 연행되는 도중 츠루에게 내가 신세계의 괴물들의 고삐를 쥐고 있고, 마리조아의 천룡인들이 네놈들을 끌어내릴거라며 언젠가 후회할 것이라고 말한다. 현재는 임펠 다운 레벨 6에 수감되어있다. 이후, 임펠 다운 레벨 6에 수감된 모습이 나오게 된다. 임펠 다운에서 자신을 지키고 있는 마젤란에게 자신이 있는 독방이 너무 외롭다며, 마리조아에 비밀 보물에 대해 입막음을 하려고 킬러를 보내왔냐며 웃으면서 말을 한다.',\n",
       " '돈키호테 도플라밍고는 어렸을 때 아버지의 결정으로 아버지, 어머니, 동생 로시난테(코라손)와 함께 성지 마리조아를 떠나 살기 시작했다. 처음에는 이전처럼 부유한 생활을 했지만, 우연히 자신이 천룡인이라는 것을 알게 된 사람들에 의해 집을 잃고 쫓겨다니는 신세가 되었다. 천룡인에 대한 사람들의 증오심은 두려움 못지않게 컸기 때문에, 도플라밍고와 그의 가족은 몇 번이고 죽을 위기를 맞았다. 도플라밍고가 8살이었을 때 어머니가 병을 앓다가 숨을 거두었고, 이 일로 아버지에 대해 분노를 느낀 도플라밍고는 10살 때 아버지를 자기 손으로 죽였다. 그리고 성지 마리조아로 돌아가 다시 천룡인으로 살려고 했으나 그들의 가족은 다른 천룡인들로부터 \\'배신자\\'로 낙인찍혀 두 번 다시 천룡인으로 살 수가 없었다. 이 이후로 도플라밍고는 천룡인을 포함한 전 세계에 대한 복수심을 품고 해적 생활을 시작했다. 해적 활동을 하던 도플라밍고는 천룡인에게 바쳐지는 공물을 나르는 배를 습격해 천상금으로 천룡인을 협박해 세계정부는 그에게 칠무해 자리를 내준다. 칠무해라는 특권인 동시에 족쇄가 되는 이름을 줘야 도플라밍고를 감시할 수 있기 때문이다. 한때 천룡인의 직함을 갖고 있었던 돈키호테 일족인지라 해군본부에서는 해당 인물에 대한 지속적인 관리가 필요했다. 한편으로 칠무해가 되기 전에는 자신의 부하 중 하나인 베르고를 해군에 입대시켜 해군 본부 중장으로 만들어 놓은 다음 베르고로 하여금 해군 본부에서 스파이를 하도록 시켰다. 13년전 코라손이 트라팔가 로에게 해군에게 도움을 요청하라고 했지만 그 많은 해군 중에서 하필이면 베르고에게 도움을 요청하는 바람에 코라손의 계획이 실패한다. 10년 전 칠무해가 된 도플라밍고는 과거에 돈키호테 일족이 다스렸던 드레스로자를 찾아가 국왕인 리쿠 왕에게 나라를 파는 조건으로 100억 베리를 요구한다. 리쿠 왕은 나라를 구하기 위해 국민들에게 돈을 내어 줄 것을 요구하고, 마침내 100억 베리가 모아진다. 하지만 도플라밍고는 리쿠 왕과 드레스로자의 병사들에게 패러사이트를 걸어 사람들을 베도록 조종하고, 사람들이 리쿠 왕에 대해 배반감을 느낄 때 \"이 나라를 구하러 왔다\"고 외치며 일당과 함께 병사들을 몰살한다. 이 일로 도플라밍고는 드레스로자 국민들의 신임을 얻어 드레스로자의 왕이 된다. 드레스로자에서 왕이 된 이후 어느 국민들에게 거짓 친절을 베푸는 반면, 또 어떤 국민들을 장난감으로 만들어 밤낮없이 일을 시키게 하여 어둠의 증오심을 불러일으킨다. 그러던 도중 우솝에 의하여, 장난감들은 본래의 인간으로 돌아가게 된다. 드레스로자에 있던 모든 진실을 모두 감추기 위해 새장을 친뒤 별☆개 1억 베리(레베카, 비올라, 킨에몬, 프랑키, 니코 로빈), 별☆☆개 2억 베리(퀴로스, 롤로노아 조로), 별☆☆☆개 3억 베리(리쿠 돌드 3세, 사보, 트라팔가 로, 몽키 D. 루피)에게 현상금을 걸고, 장난감들을 해방시키고 이 게임을 만든 장본인 우솝에게 별☆☆☆☆☆개 5억 베리를 걸게 만든다. 그러다 자신을 제외한 돈키호테 패밀리가 모두 붕괴되자, 계획을 바꿔 새장을 수축시켜서 모든 국민들을 죽이려다 루피와 싸우게 된다. 한참을 싸웠지만 루피의 기어4 기술을 맞고 고전을 면치 못하다가 결국 루피의 기어 4 킹콩 건을 맞고 쓰러진다. 그 다음에 해군에게 체포된다. 임펠 다운으로 연행되는 도중 츠루에게 내가 신세계의 괴물들의 고삐를 쥐고 있고, 마리조아의 천룡인들이 네놈들을 끌어내릴거라며 언젠가 후회할 것이라고 말한다. 현재는 임펠 다운 레벨 6에 수감되어있다. 이후, 임펠 다운 레벨 6에 수감된 모습이 나오게 된다. 임펠 다운에서 자신을 지키고 있는 마젤란에게 자신이 있는 독방이 너무 외롭다며, 마리조아에 비밀 보물에 대해 입막음을 하려고 킬러를 보내왔냐며 웃으면서 말을 한다.',\n",
       " '돈키호테 도플라밍고는 어렸을 때 아버지의 결정으로 아버지, 어머니, 동생 로시난테(코라손)와 함께 성지 마리조아를 떠나 살기 시작했다. 처음에는 이전처럼 부유한 생활을 했지만, 우연히 자신이 천룡인이라는 것을 알게 된 사람들에 의해 집을 잃고 쫓겨다니는 신세가 되었다. 천룡인에 대한 사람들의 증오심은 두려움 못지않게 컸기 때문에, 도플라밍고와 그의 가족은 몇 번이고 죽을 위기를 맞았다. 도플라밍고가 8살이었을 때 어머니가 병을 앓다가 숨을 거두었고, 이 일로 아버지에 대해 분노를 느낀 도플라밍고는 10살 때 아버지를 자기 손으로 죽였다. 그리고 성지 마리조아로 돌아가 다시 천룡인으로 살려고 했으나 그들의 가족은 다른 천룡인들로부터 \\'배신자\\'로 낙인찍혀 두 번 다시 천룡인으로 살 수가 없었다. 이 이후로 도플라밍고는 천룡인을 포함한 전 세계에 대한 복수심을 품고 해적 생활을 시작했다. 해적 활동을 하던 도플라밍고는 천룡인에게 바쳐지는 공물을 나르는 배를 습격해 천상금으로 천룡인을 협박해 세계정부는 그에게 칠무해 자리를 내준다. 칠무해라는 특권인 동시에 족쇄가 되는 이름을 줘야 도플라밍고를 감시할 수 있기 때문이다. 한때 천룡인의 직함을 갖고 있었던 돈키호테 일족인지라 해군본부에서는 해당 인물에 대한 지속적인 관리가 필요했다. 한편으로 칠무해가 되기 전에는 자신의 부하 중 하나인 베르고를 해군에 입대시켜 해군 본부 중장으로 만들어 놓은 다음 베르고로 하여금 해군 본부에서 스파이를 하도록 시켰다. 13년전 코라손이 트라팔가 로에게 해군에게 도움을 요청하라고 했지만 그 많은 해군 중에서 하필이면 베르고에게 도움을 요청하는 바람에 코라손의 계획이 실패한다. 10년 전 칠무해가 된 도플라밍고는 과거에 돈키호테 일족이 다스렸던 드레스로자를 찾아가 국왕인 리쿠 왕에게 나라를 파는 조건으로 100억 베리를 요구한다. 리쿠 왕은 나라를 구하기 위해 국민들에게 돈을 내어 줄 것을 요구하고, 마침내 100억 베리가 모아진다. 하지만 도플라밍고는 리쿠 왕과 드레스로자의 병사들에게 패러사이트를 걸어 사람들을 베도록 조종하고, 사람들이 리쿠 왕에 대해 배반감을 느낄 때 \"이 나라를 구하러 왔다\"고 외치며 일당과 함께 병사들을 몰살한다. 이 일로 도플라밍고는 드레스로자 국민들의 신임을 얻어 드레스로자의 왕이 된다. 드레스로자에서 왕이 된 이후 어느 국민들에게 거짓 친절을 베푸는 반면, 또 어떤 국민들을 장난감으로 만들어 밤낮없이 일을 시키게 하여 어둠의 증오심을 불러일으킨다. 그러던 도중 우솝에 의하여, 장난감들은 본래의 인간으로 돌아가게 된다. 드레스로자에 있던 모든 진실을 모두 감추기 위해 새장을 친뒤 별☆개 1억 베리(레베카, 비올라, 킨에몬, 프랑키, 니코 로빈), 별☆☆개 2억 베리(퀴로스, 롤로노아 조로), 별☆☆☆개 3억 베리(리쿠 돌드 3세, 사보, 트라팔가 로, 몽키 D. 루피)에게 현상금을 걸고, 장난감들을 해방시키고 이 게임을 만든 장본인 우솝에게 별☆☆☆☆☆개 5억 베리를 걸게 만든다. 그러다 자신을 제외한 돈키호테 패밀리가 모두 붕괴되자, 계획을 바꿔 새장을 수축시켜서 모든 국민들을 죽이려다 루피와 싸우게 된다. 한참을 싸웠지만 루피의 기어4 기술을 맞고 고전을 면치 못하다가 결국 루피의 기어 4 킹콩 건을 맞고 쓰러진다. 그 다음에 해군에게 체포된다. 임펠 다운으로 연행되는 도중 츠루에게 내가 신세계의 괴물들의 고삐를 쥐고 있고, 마리조아의 천룡인들이 네놈들을 끌어내릴거라며 언젠가 후회할 것이라고 말한다. 현재는 임펠 다운 레벨 6에 수감되어있다. 이후, 임펠 다운 레벨 6에 수감된 모습이 나오게 된다. 임펠 다운에서 자신을 지키고 있는 마젤란에게 자신이 있는 독방이 너무 외롭다며, 마리조아에 비밀 보물에 대해 입막음을 하려고 킬러를 보내왔냐며 웃으면서 말을 한다.',\n",
       " '돈키호테 도플라밍고는 어렸을 때 아버지의 결정으로 아버지, 어머니, 동생 로시난테(코라손)와 함께 성지 마리조아를 떠나 살기 시작했다. 처음에는 이전처럼 부유한 생활을 했지만, 우연히 자신이 천룡인이라는 것을 알게 된 사람들에 의해 집을 잃고 쫓겨다니는 신세가 되었다. 천룡인에 대한 사람들의 증오심은 두려움 못지않게 컸기 때문에, 도플라밍고와 그의 가족은 몇 번이고 죽을 위기를 맞았다. 도플라밍고가 8살이었을 때 어머니가 병을 앓다가 숨을 거두었고, 이 일로 아버지에 대해 분노를 느낀 도플라밍고는 10살 때 아버지를 자기 손으로 죽였다. 그리고 성지 마리조아로 돌아가 다시 천룡인으로 살려고 했으나 그들의 가족은 다른 천룡인들로부터 \\'배신자\\'로 낙인찍혀 두 번 다시 천룡인으로 살 수가 없었다. 이 이후로 도플라밍고는 천룡인을 포함한 전 세계에 대한 복수심을 품고 해적 생활을 시작했다. 해적 활동을 하던 도플라밍고는 천룡인에게 바쳐지는 공물을 나르는 배를 습격해 천상금으로 천룡인을 협박해 세계정부는 그에게 칠무해 자리를 내준다. 칠무해라는 특권인 동시에 족쇄가 되는 이름을 줘야 도플라밍고를 감시할 수 있기 때문이다. 한때 천룡인의 직함을 갖고 있었던 돈키호테 일족인지라 해군본부에서는 해당 인물에 대한 지속적인 관리가 필요했다. 한편으로 칠무해가 되기 전에는 자신의 부하 중 하나인 베르고를 해군에 입대시켜 해군 본부 중장으로 만들어 놓은 다음 베르고로 하여금 해군 본부에서 스파이를 하도록 시켰다. 13년전 코라손이 트라팔가 로에게 해군에게 도움을 요청하라고 했지만 그 많은 해군 중에서 하필이면 베르고에게 도움을 요청하는 바람에 코라손의 계획이 실패한다. 10년 전 칠무해가 된 도플라밍고는 과거에 돈키호테 일족이 다스렸던 드레스로자를 찾아가 국왕인 리쿠 왕에게 나라를 파는 조건으로 100억 베리를 요구한다. 리쿠 왕은 나라를 구하기 위해 국민들에게 돈을 내어 줄 것을 요구하고, 마침내 100억 베리가 모아진다. 하지만 도플라밍고는 리쿠 왕과 드레스로자의 병사들에게 패러사이트를 걸어 사람들을 베도록 조종하고, 사람들이 리쿠 왕에 대해 배반감을 느낄 때 \"이 나라를 구하러 왔다\"고 외치며 일당과 함께 병사들을 몰살한다. 이 일로 도플라밍고는 드레스로자 국민들의 신임을 얻어 드레스로자의 왕이 된다. 드레스로자에서 왕이 된 이후 어느 국민들에게 거짓 친절을 베푸는 반면, 또 어떤 국민들을 장난감으로 만들어 밤낮없이 일을 시키게 하여 어둠의 증오심을 불러일으킨다. 그러던 도중 우솝에 의하여, 장난감들은 본래의 인간으로 돌아가게 된다. 드레스로자에 있던 모든 진실을 모두 감추기 위해 새장을 친뒤 별☆개 1억 베리(레베카, 비올라, 킨에몬, 프랑키, 니코 로빈), 별☆☆개 2억 베리(퀴로스, 롤로노아 조로), 별☆☆☆개 3억 베리(리쿠 돌드 3세, 사보, 트라팔가 로, 몽키 D. 루피)에게 현상금을 걸고, 장난감들을 해방시키고 이 게임을 만든 장본인 우솝에게 별☆☆☆☆☆개 5억 베리를 걸게 만든다. 그러다 자신을 제외한 돈키호테 패밀리가 모두 붕괴되자, 계획을 바꿔 새장을 수축시켜서 모든 국민들을 죽이려다 루피와 싸우게 된다. 한참을 싸웠지만 루피의 기어4 기술을 맞고 고전을 면치 못하다가 결국 루피의 기어 4 킹콩 건을 맞고 쓰러진다. 그 다음에 해군에게 체포된다. 임펠 다운으로 연행되는 도중 츠루에게 내가 신세계의 괴물들의 고삐를 쥐고 있고, 마리조아의 천룡인들이 네놈들을 끌어내릴거라며 언젠가 후회할 것이라고 말한다. 현재는 임펠 다운 레벨 6에 수감되어있다. 이후, 임펠 다운 레벨 6에 수감된 모습이 나오게 된다. 임펠 다운에서 자신을 지키고 있는 마젤란에게 자신이 있는 독방이 너무 외롭다며, 마리조아에 비밀 보물에 대해 입막음을 하려고 킬러를 보내왔냐며 웃으면서 말을 한다.',\n",
       " '돈키호테 도플라밍고는 어렸을 때 아버지의 결정으로 아버지, 어머니, 동생 로시난테(코라손)와 함께 성지 마리조아를 떠나 살기 시작했다. 처음에는 이전처럼 부유한 생활을 했지만, 우연히 자신이 천룡인이라는 것을 알게 된 사람들에 의해 집을 잃고 쫓겨다니는 신세가 되었다. 천룡인에 대한 사람들의 증오심은 두려움 못지않게 컸기 때문에, 도플라밍고와 그의 가족은 몇 번이고 죽을 위기를 맞았다. 도플라밍고가 8살이었을 때 어머니가 병을 앓다가 숨을 거두었고, 이 일로 아버지에 대해 분노를 느낀 도플라밍고는 10살 때 아버지를 자기 손으로 죽였다. 그리고 성지 마리조아로 돌아가 다시 천룡인으로 살려고 했으나 그들의 가족은 다른 천룡인들로부터 \\'배신자\\'로 낙인찍혀 두 번 다시 천룡인으로 살 수가 없었다. 이 이후로 도플라밍고는 천룡인을 포함한 전 세계에 대한 복수심을 품고 해적 생활을 시작했다. 해적 활동을 하던 도플라밍고는 천룡인에게 바쳐지는 공물을 나르는 배를 습격해 천상금으로 천룡인을 협박해 세계정부는 그에게 칠무해 자리를 내준다. 칠무해라는 특권인 동시에 족쇄가 되는 이름을 줘야 도플라밍고를 감시할 수 있기 때문이다. 한때 천룡인의 직함을 갖고 있었던 돈키호테 일족인지라 해군본부에서는 해당 인물에 대한 지속적인 관리가 필요했다. 한편으로 칠무해가 되기 전에는 자신의 부하 중 하나인 베르고를 해군에 입대시켜 해군 본부 중장으로 만들어 놓은 다음 베르고로 하여금 해군 본부에서 스파이를 하도록 시켰다. 13년전 코라손이 트라팔가 로에게 해군에게 도움을 요청하라고 했지만 그 많은 해군 중에서 하필이면 베르고에게 도움을 요청하는 바람에 코라손의 계획이 실패한다. 10년 전 칠무해가 된 도플라밍고는 과거에 돈키호테 일족이 다스렸던 드레스로자를 찾아가 국왕인 리쿠 왕에게 나라를 파는 조건으로 100억 베리를 요구한다. 리쿠 왕은 나라를 구하기 위해 국민들에게 돈을 내어 줄 것을 요구하고, 마침내 100억 베리가 모아진다. 하지만 도플라밍고는 리쿠 왕과 드레스로자의 병사들에게 패러사이트를 걸어 사람들을 베도록 조종하고, 사람들이 리쿠 왕에 대해 배반감을 느낄 때 \"이 나라를 구하러 왔다\"고 외치며 일당과 함께 병사들을 몰살한다. 이 일로 도플라밍고는 드레스로자 국민들의 신임을 얻어 드레스로자의 왕이 된다. 드레스로자에서 왕이 된 이후 어느 국민들에게 거짓 친절을 베푸는 반면, 또 어떤 국민들을 장난감으로 만들어 밤낮없이 일을 시키게 하여 어둠의 증오심을 불러일으킨다. 그러던 도중 우솝에 의하여, 장난감들은 본래의 인간으로 돌아가게 된다. 드레스로자에 있던 모든 진실을 모두 감추기 위해 새장을 친뒤 별☆개 1억 베리(레베카, 비올라, 킨에몬, 프랑키, 니코 로빈), 별☆☆개 2억 베리(퀴로스, 롤로노아 조로), 별☆☆☆개 3억 베리(리쿠 돌드 3세, 사보, 트라팔가 로, 몽키 D. 루피)에게 현상금을 걸고, 장난감들을 해방시키고 이 게임을 만든 장본인 우솝에게 별☆☆☆☆☆개 5억 베리를 걸게 만든다. 그러다 자신을 제외한 돈키호테 패밀리가 모두 붕괴되자, 계획을 바꿔 새장을 수축시켜서 모든 국민들을 죽이려다 루피와 싸우게 된다. 한참을 싸웠지만 루피의 기어4 기술을 맞고 고전을 면치 못하다가 결국 루피의 기어 4 킹콩 건을 맞고 쓰러진다. 그 다음에 해군에게 체포된다. 임펠 다운으로 연행되는 도중 츠루에게 내가 신세계의 괴물들의 고삐를 쥐고 있고, 마리조아의 천룡인들이 네놈들을 끌어내릴거라며 언젠가 후회할 것이라고 말한다. 현재는 임펠 다운 레벨 6에 수감되어있다. 이후, 임펠 다운 레벨 6에 수감된 모습이 나오게 된다. 임펠 다운에서 자신을 지키고 있는 마젤란에게 자신이 있는 독방이 너무 외롭다며, 마리조아에 비밀 보물에 대해 입막음을 하려고 킬러를 보내왔냐며 웃으면서 말을 한다.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac68cbaab3d35a4051d1e6f867fd4af3051b6908d059b1a63e6857e32e52681"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('mrc_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
