{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments, RobertaModel, RobertaPreTrainedModel, BertModel, BertPreTrainedModel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwer\n",
    "class DenseRetrieval_with_Faiss:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder,\n",
    "        num_neg=5,\n",
    "        hard_neg=1,\n",
    "        is_trained=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        학습과 추론에 사용될 여러 셋업을 마쳐봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "        \n",
    "        self.wiki_dataset = load_from_disk(\"/home/ubuntu/workspace/data/wiki_preprocessed_droped\")\n",
    "        \n",
    "        if is_trained:\n",
    "            pass\n",
    "        else:\n",
    "            self.prepare_in_batch_negative(num_neg=num_neg, hard_neg=hard_neg)\n",
    "\n",
    "    def prepare_in_batch_negative(self,\n",
    "        dataset=None,\n",
    "        num_neg=5,\n",
    "        hard_neg=1,\n",
    "        k=100,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        if num_neg < hard_neg:\n",
    "            raise 'num_neg는 hard_neg보다 커야합니다.'\n",
    "        wiki_datasets = self.wiki_dataset\n",
    "        wiki_datasets.load_elasticsearch_index(\"text\", host=\"localhost\", port=\"9200\", es_index_name=\"wikipedia_contexts\")\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # 1. In-Batch-Negative 만들기\n",
    "        # CORPUS를 np.array로 변환해줍니다.\n",
    "        p_with_neg = []\n",
    "\n",
    "        for c in tqdm(dataset):\n",
    "            p_with_neg.append(c['context'])\n",
    "            query = c['question']\n",
    "            p_neg = []\n",
    "            _, retrieved_examples = wiki_datasets.get_nearest_examples(\"text\", query, k=k)\n",
    "            for index in range(k):\n",
    "                if retrieved_examples['document_id'][index] == c['document_id']:\n",
    "                    continue\n",
    "                p_neg.append(retrieved_examples['text'][index])\n",
    "            p_with_neg.extend(p_neg[:hard_neg])\n",
    "            p_with_neg.extend(p_neg[k - num_neg + hard_neg:])\n",
    "\n",
    "        # 2. (Question, Passage) 데이터셋 만들어주기\n",
    "        q_seqs = tokenizer(\n",
    "            dataset[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        p_seqs = tokenizer(\n",
    "            p_with_neg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg+1, max_len)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "        valid_seqs = tokenizer(\n",
    "            dataset[\"context\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        passage_dataset = TensorDataset(\n",
    "            valid_seqs[\"input_ids\"],\n",
    "            valid_seqs[\"attention_mask\"],\n",
    "            valid_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        self.passage_dataloader = DataLoader(\n",
    "            passage_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "    def build_faiss(self, num_clusters=16):\n",
    "  \n",
    "        \"\"\"\n",
    "        Note:\n",
    "            위에서 Faiss를 사용했던 기억을 떠올려보면,\n",
    "            Indexer를 구성해서 .search() 메소드를 활용했습니다.\n",
    "            여기서는 Indexer 구성을 해주도록 합시다.\n",
    "        \"\"\"\n",
    "        eval_batch_size = 8\n",
    "\n",
    "        self.search_corpus = list(set([example['text'] for example in self.wiki_dataset]))\n",
    "        p_encoder = self.p_encoder\n",
    "        \n",
    "        # Construt dataloader\n",
    "        valid_p_seqs = self.tokenizer(\n",
    "            self.search_corpus,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        valid_dataset = TensorDataset(\n",
    "            valid_p_seqs[\"input_ids\"],\n",
    "            valid_p_seqs[\"attention_mask\"],\n",
    "            valid_p_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        valid_sampler = SequentialSampler(valid_dataset)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler=valid_sampler,\n",
    "            batch_size=eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Inference using the passage encoder to get dense embeddeings\n",
    "        p_embs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            epoch_iterator = tqdm(\n",
    "                valid_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                position=0,\n",
    "                leave=True\n",
    "            )\n",
    "            p_encoder.eval()\n",
    "\n",
    "            for _, batch in enumerate(epoch_iterator):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2]\n",
    "                }\n",
    "                \n",
    "                outputs = p_encoder(**p_inputs).to(\"cpu\").numpy()\n",
    "                p_embs.extend(outputs)\n",
    "        p_embs = np.array(p_embs)\n",
    "        emb_dim = p_embs.shape[-1]\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "        self.indexer = faiss.IndexIVFScalarQuantizer(\n",
    "            quantizer,\n",
    "            quantizer.d,\n",
    "            num_clusters,\n",
    "            faiss.METRIC_L2\n",
    "        )\n",
    "        self.indexer.train(p_embs)\n",
    "        self.indexer.add(p_embs)\n",
    "\n",
    "    def train(self, args=None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        batch_size = args.per_device_train_batch_size\n",
    "\n",
    "        # Optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon\n",
    "        )\n",
    "        t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Start training!\n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\")\n",
    "        # for _ in range(int(args.num_train_epochs)):\n",
    "        for _ in train_iterator:\n",
    "\n",
    "            with tqdm(self.train_dataloader, unit=\"batch\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "\n",
    "                    self.p_encoder.train()\n",
    "                    self.q_encoder.train()\n",
    "            \n",
    "                    targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                    targets = targets.to(args.device)\n",
    "\n",
    "                    p_inputs = {\n",
    "                        \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"token_type_ids\": batch[2].view(batch_size * (self.num_neg + 1), -1).to(args.device)\n",
    "                    }\n",
    "            \n",
    "                    q_inputs = {\n",
    "                        \"input_ids\": batch[3].to(args.device),\n",
    "                        \"attention_mask\": batch[4].to(args.device),\n",
    "                        \"token_type_ids\": batch[5].to(args.device)\n",
    "                    }\n",
    "\n",
    "                    # (batch_size*(num_neg+1), emb_dim)\n",
    "                    p_outputs = self.p_encoder(**p_inputs)\n",
    "                    # (batch_size*, emb_dim)\n",
    "                    q_outputs = self.q_encoder(**q_inputs)\n",
    "\n",
    "                    # Calculate similarity score & loss\n",
    "                    p_outputs_t = torch.transpose(p_outputs.view(batch_size, self.num_neg + 1, -1), 1 , 2)\n",
    "                    q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                    sim_scores = torch.bmm(q_outputs, p_outputs_t).squeeze()  #(batch_size, num_neg + 1)\n",
    "                    sim_scores = sim_scores.view(batch_size, -1)\n",
    "                    sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                    loss = F.nll_loss(sim_scores, targets)\n",
    "                    tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    self.p_encoder.zero_grad()\n",
    "                    self.q_encoder.zero_grad()\n",
    "\n",
    "                    global_step += 1\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    del p_inputs, q_inputs\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (torch.Tensor):\n",
    "                Dense Representation으로 표현된 query를 받습니다.\n",
    "                문자열이 아님에 주의합시다.\n",
    "            k (int, default=1):\n",
    "                상위 몇 개의 유사한 passage를 뽑을 것인지 결정합니다.\n",
    "\n",
    "        Note:\n",
    "            받은 query를 이 객체에 저장된 indexer를 활용해서\n",
    "            유사한 문서를 찾아봅시다.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_q_seqs = self.tokenizer(query, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_encoder.eval()\n",
    "            q_emb = q_encoder(**valid_q_seqs).to(\"cpu\").numpy()\n",
    "        \n",
    "        q_emb = q_emb.astype(np.float32)\n",
    "        D, I = self.indexer.search(q_emb, k)\n",
    "        distances, index = D.tolist()[0], I.tolist()[0]\n",
    "        \n",
    "        distance_list, doc_list = [], []\n",
    "        for d, i in zip(distances, index):\n",
    "            distance_list.append(d)\n",
    "            doc_list.append(self.search_corpus[i])\n",
    "\n",
    "        return distance_list, doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEncoder, self).__init__(config)\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 안하니 검증데이터와 훈련데이터를 합칩니다.\n",
    "train_dataset = load_from_disk('/home/ubuntu/workspace/data/train_dataset')\n",
    "train = train_dataset['train'].to_dict()\n",
    "valid = train_dataset['validation'].to_dict()\n",
    "for key in train.keys():\n",
    "  train[key].extend(valid[key])\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DenseRetrieval_with_Faiss(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder,\n",
    "    is_trained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fe834b4d914f47b35fd7e979faff90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f8f8d9b2bf4d81ac1e8108e65e592f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2096 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870bd992653e4c71aa72345fb0d0f937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2096 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63b403ac5d94bf0bbabc3a03b9f54ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6996 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.build_faiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "query = \"도플라밍고를 무찔렀으며 3억베리의 현상금을 가진 인물은?\"\n",
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac68cbaab3d35a4051d1e6f867fd4af3051b6908d059b1a63e6857e32e52681"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('mrc_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
